
# See LICENSE for license details.

# This file is automatically generated. Do not edit.

#*****************************************************************************
# vfncvt.x.f.w_LMUL2SEW32.S
#-----------------------------------------------------------------------------
#
# Test vfncvt.x.f.w insnructions.
# With LMUL=2, SEW=32
#

#include "riscv_test.h"
#include "test_macros.h"

RVTEST_RV64UV

RVTEST_CODE_BEGIN


  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, 128
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, 128
  vsetvli t1, t0, e32,m2,tu,ma
  vfncvt.x.f.w v4, v8

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 128
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8, v0.t

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 128
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8, v0.t

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, 255
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, 255
  vsetvli t1, t0, e32,m2,tu,ma
  vfncvt.x.f.w v4, v8

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 255
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8, v0.t

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 255
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8, v0.t

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, 256
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, 256
  vsetvli t1, t0, e32,m2,tu,ma
  vfncvt.x.f.w v4, v8

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 256
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8, v0.t

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8

  li t0, -1
  la a2, tdat
  vsetvli t1, t0, e32,m2,ta,ma
  vle32.v v4, (a2)
  la a2, tdat+8

  vsetvli t1, t0, e32,m4,ta,ma
  vle32.v v8, (a2)

  
  li t0, -1
  vsetvli t1, t0, e8,m1,ta,ma
  la a3, mask
  vle8.v v0, (a3)

  li t0, 256
  vsetvli t1, t0, e32,m2,ta,ma
  vfncvt.x.f.w v4, v8, v0.t

  li t0, -1
  vsetvli t1, t0, e32,m2,ta,ma
  la a1, res
  vse32.v v4, (a1)

  addi x0, x4, 8


  TEST_CASE(2, x0, 0x0)
  TEST_PASSFAIL

RVTEST_CODE_END

  .data
RVTEST_DATA_BEGIN

res:
  .zero 4112

tdat:
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1
  .quad 0x1
  .quad 0x3fffffff8
  .quad 0x700000000
  .quad 0xffffffffefffffff
  .quad 0x100000000
  .quad 0xefffffffffffffff
  .quad 0x100000000
  .quad 0x1

mask:
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555
  .quad 0x5555555555555555

RVTEST_DATA_END
